from fastapi import FastAPI
from pydantic import BaseModel
from typing import List, Literal
from langchain_openai import ChatOpenAI
from langchain_upstage import UpstageEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import os
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
openai_key = os.getenv("OPENAI_API_KEY")
upstage_key = os.getenv("UPSTAGE_API_KEY")
pinecone_key = os.getenv("PINECONE_API_KEY")

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ë˜ëŠ” .envë¡œ ê´€ë¦¬ ê°€ëŠ¥)
os.environ["OPENAI_API_KEY"] = openai_key
os.environ["UPSTAGE_API_KEY"] = upstage_key
os.environ["PINECONE_API_KEY"] = pinecone_key

# FastAPI ì•± ìƒì„±
app = FastAPI()

# ChatMessage í´ë˜ìŠ¤ ì •ì˜
class ChatMessage(BaseModel):
    role: Literal["user", "assistant"]
    content: str

# ìš”ì²­ ëª¨ë¸
class RagRequest(BaseModel):
    question: str
    game: str  # ì˜ˆ: "katan", "splendor"
    history: List[ChatMessage] = []  # ì´ì „ ëŒ€í™” ê¸°ë¡

# API ì—”ë“œí¬ì¸íŠ¸
@app.post("/api/rag")
def answer_rag(request: RagRequest):
    # 1. LLM ë° ì„ë² ë”© ëª¨ë¸
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
    embedding = UpstageEmbeddings(model="solar-embedding-1-large")

    vectorstore = PineconeVectorStore.from_existing_index(
        index_name="boardgame-rag",
        embedding=embedding,
        namespace=request.game
    )

    # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    docs = vectorstore.similarity_search(request.question, k=5)

    # # 2. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    # retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
    # docs = retriever.invoke(request.question)
    # context = "\n".join([doc.page_content for doc in docs])

    # print(context)

    # 3. í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ êµ¬ì„±
    messages = [("system", 
        "ë„ˆëŠ” ë³´ë“œê²Œì„ ë£°ì„ ì„¤ëª…í•´ì£¼ëŠ” AIì•¼. ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ì¡´ëŒ“ë§ë¡œ ì„¤ëª…í•´ì¤˜. "
        "ì‚¬ìš©ìê°€ ë³´ë‚´ì¤€ ë¬¸ì„œì— ìˆëŠ” ë‚´ìš©ìœ¼ë¡œë§Œ ë‹µë³€í•˜ê³ , ë‚´ìš©ì´ ì—†ë‹¤ë©´ ì˜ ëª¨ë¥´ê² ë‹¤ê³  ë§í•´."
        "ë§Œì•½ ì‚¬ìš©ìê°€ ë³´ë‚´ì¤€ ë¬¸ì„œê°€ ì—†ìœ¼ë©´ ì•„ëŠ” ë²”ìœ„ ë‚´ë¡œ ëŒ€ë‹µí•´ ì£¼ê³  ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•´ì¤˜")]

    # history ë°˜ì˜
    for msg in request.history:
        messages.append((msg.role, msg.content))
    
    # ë¬¸ì„œ ê¸°ë°˜ RAG ë‹µë³€ vs ì¼ë°˜ LLM ë‹µë³€
    if docs:
        context = "\n".join([doc.page_content for doc in docs])
        print(context)
        messages.append(("human", f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.\n\në¬¸ì„œ: {context}\n\nì§ˆë¬¸: {request.question}"))
    else:
        print(request.game)
        print("LLMë§Œì˜ ë‹µë³€")  
        messages.append(("human", f"'{request.game}' ê²Œì„ì— ëŒ€í•œ ë¬¸ì„œëŠ” ì—†ì§€ë§Œ, ë„¤ê°€ ì•„ëŠ” ë²”ìœ„ ë‚´ì—ì„œ ë‹µë³€í•´ì¤˜. "
                                  f"ì •í™•í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì˜ ëª¨ë¥´ê² ë‹¤ê³  ë‹µë³€í•´ì•¼ í•´. ì§ˆë¬¸: {request.question}"))
        
    # # ë§ˆì§€ë§‰ ì§ˆë¬¸ + context
    # messages.append(("human", f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€í•´ì¤˜.\n\në¬¸ì„œ: {context}\n\nì§ˆë¬¸: {request.question}"))

    # í”„ë¡¬í”„íŠ¸ ì²´ì¸ êµ¬ì„±
    prompt = ChatPromptTemplate.from_messages(messages)
    chain = prompt | llm | StrOutputParser()
    answer = chain.invoke({})  # context, question ëª¨ë‘ ë©”ì‹œì§€ ì•ˆì— í¬í•¨ë¨

    return {"answer": answer}


@app.get("/")
def root():
    return {"message": "ğŸ² Boardgame RAG API is running!"}

# ì„œë²„ ì‹¤í–‰ ì½”ë“œ
# uvicorn rag_server:app --reload --port 8000